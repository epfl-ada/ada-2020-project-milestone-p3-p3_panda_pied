{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Republication report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we attempt replicating the figure 2.a from the article \"Friendship and Mobility: User Movement in Location-Based Social Networks \". The figure represents the relation between the probability of friendship and the distance between their houses. To replicate it, we will first find the position of the houses of each user by using the method described in the article and then assign a distance between each user and his friends. Finally, we will be able to calculate the probability of occurrence for all the distance and replicate the figure. More details are provided in the following sections. The article can be found at this address: https://drive.google.com/drive/folders/1QRoC6DAMoD_BxJ6KPMdijBRTDUYhoVfG?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # needed for plotting\n",
    "import geopandas as gpd # for Geo-location filtering\n",
    "from netCDF4 import Dataset as nc  # for loading standard climate date format (nc extension)\n",
    "import netCDF4 # for loading standard climate date format (nc extension)\n",
    "import xarray as xr # for handling climate data\n",
    "from pandarallel import pandarallel # for running pandas functions in parallel\n",
    "import multiprocessing # for general parallelizing of codes\n",
    "import tqdm # for having progres bar\n",
    "pandarallel.initialize(nb_workers=multiprocessing.cpu_count()-1)\n",
    "from geopandas.tools import sjoin\n",
    "from tqdm import tqdm # package used for having progres bar\n",
    "tqdm.pandas()\n",
    "from datetime import datetime, date # package to thandle date objects\n",
    "from scipy import stats # used for statistic analysis\n",
    "import holidays # package used to obtain holidays\n",
    "from timezonefinder import TimezoneFinderL # package used to convert time accroding to timezones\n",
    "tf = TimezoneFinderL(in_memory=False)\n",
    "from pytz import timezone # package used to convert to recognize timezones from location\n",
    "import pytz\n",
    "utc = pytz.utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes\n",
    "### Pandarallel and multiprocessing\n",
    "1. considering the large datasets that we deal with (e.g. temperature by date in each location on US which is a 3d grided data) we were urged to run our analysis and especially data wrangling using full power of our machine up to its capacity. Therefore, we are runing our operations in parallel and ```multiprocessing``` and ```pandarallel``` were used to do so \n",
    "2. The ```NetCDF``` package is inprted to handle ***nc*** file extension which is the standard extension for historical climate data. Our data was obtained from:\n",
    "\n",
    "     https://www.northwestknowledge.net/metdata/ \n",
    "\n",
    "     which is provided by University of Idaho. The data needs to be processed before being used in our analysis.\n",
    "\n",
    "3. ```xarray``` package is used to handle multi-dimensional arrays such as climate data\n",
    "\n",
    "4. ```tqdm``` let's us have cool progress bat on our loops\n",
    "\n",
    "5. ```timezonefinder``` and ```pytz``` are used to handle time zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of two set of data\n",
    "DATA_FOLDER = 'Data/'\n",
    "\n",
    "INP_FOLDER = 'InputData/'\n",
    "\n",
    "# Friendship undirected network users and Time and location information of check-ins made by users\n",
    "\n",
    "#Brightkite\n",
    "BRIGHT_NETWORK_DATASET = DATA_FOLDER + \"Brightkite_edges.txt.gz\"\n",
    "BRIGHT_CHECKIN_TIME_LOCATION_DATASET = DATA_FOLDER + \"Brightkite_totalCheckins.txt.gz\"\n",
    "\n",
    "#Gowalla\n",
    "GOWALLA_NETWORK_DATASET = DATA_FOLDER + \"Gowalla_edges.txt.gz\"\n",
    "GOWALLA_CHECKIN_TIME_LOCATION_DATASET = DATA_FOLDER + \"Gowalla_totalCheckins.txt.gz\"\n",
    "\n",
    "# loading all the data set\n",
    "b_network_df = pd.read_csv(BRIGHT_NETWORK_DATASET, delimiter=\"\\t\",\n",
    "                                    error_bad_lines =False, header = 0 )\n",
    "b_checkin_df = pd.read_csv(BRIGHT_CHECKIN_TIME_LOCATION_DATASET, delimiter=\"\\t\",\n",
    "                                       error_bad_lines =False, header = 0 )\n",
    "g_network_df = pd.read_csv(GOWALLA_NETWORK_DATASET, delimiter = \"\\t\",header = None)\n",
    "g_checkin_df = pd.read_csv(GOWALLA_CHECKIN_TIME_LOCATION_DATASET, delimiter = \"\\t\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the header \n",
    "NETWORK_COLUMNS = ['user','friend_edge']\n",
    "CHECKIN_COLUMNS = ['user','checkin_time','latitude','longitude','location_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "b_network_df.columns = NETWORK_COLUMNS\n",
    "b_checkin_df.columns = CHECKIN_COLUMNS\n",
    "g_network_df.columns = NETWORK_COLUMNS\n",
    "g_checkin_df.columns = CHECKIN_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_network_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_network_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, before starting our recollection, we need to pre-process our dataset. So, we can begin by counting all the null values and remove them if necessary. We also want the latitude and longitude to be respectively in the interval [-90,90] and [-180,180]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_checkin_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cleaning the data'''\n",
    "\n",
    "# locate rows with NaN\n",
    "rows_with_NaN_b = b_checkin_df.loc[(b_checkin_df['latitude'].isna() == True)]\n",
    "rows_with_NaN_g = g_checkin_df.loc[(g_checkin_df['latitude'].isna() == True)]\n",
    "\n",
    "# locate rows that are not in those interval : -90 < Latitude < 90 and -180 < Longitude < 180\n",
    "\n",
    "# latitude\n",
    "lat_to_removed_b = b_checkin_df.loc[(b_checkin_df['latitude'] > 90) | (b_checkin_df['latitude'] < -90)]\n",
    "lat_to_removed_g = g_checkin_df.loc[(g_checkin_df['latitude'] > 90) | (g_checkin_df['latitude'] < -90)]\n",
    "\n",
    "# longitude\n",
    "long_to_removed_b = b_checkin_df.loc[(b_checkin_df['longitude'] > 180) | (b_checkin_df['longitude'] < -180)]\n",
    "long_to_removed_g = g_checkin_df.loc[(g_checkin_df['longitude'] > 180) | (g_checkin_df['longitude'] < -180)]\n",
    "\n",
    "# Dropping all the rows\n",
    "\n",
    "# Brightkite\n",
    "b_checkin_df = b_checkin_df.drop(rows_with_NaN_b.index)\n",
    "b_checkin_df = b_checkin_df.drop(lat_to_removed_b.index)\n",
    "b_checkin_df = b_checkin_df.drop(long_to_removed_b.index)\n",
    "\n",
    "#see bellow\n",
    "b_checkin_df = b_checkin_df.drop(b_checkin_df[b_checkin_df['location_id'] == '00000000000000000000000000000000'].index)\n",
    "\n",
    "# Gowalla\n",
    "g_checkin_df = g_checkin_df.drop(rows_with_NaN_g.index)\n",
    "g_checkin_df = g_checkin_df.drop(lat_to_removed_g.index)\n",
    "g_checkin_df = g_checkin_df.drop(long_to_removed_g.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some weird values were spotted for the bright kite dataset on the position (0,0) with a \"location_id\" equal to \" 00000...0000 \". As quoted on page 1083 of our article: \" The total number of check-ins for Gowalla is 6.4 million and 4.5 million for Brightkite. \" So to be more consistent with those numbers we decide to also remove them. We can now show the shape of our two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of the check-ins used in the recollection \n",
    "print('The total number of check-ins for Gowalla :',g_checkin_df.shape[0])\n",
    "print('The total number of check-ins for Brightkite :',b_checkin_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Recollection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classing every check-ins in cells of 25x25 km \n",
    "In the next two sections, we will provide more details on the method described in the paper.\n",
    "So, we begin by classing every check-in in a grid of 25x25 km over the world. One way of classing into cells is to perform euclidean division by 25 on both latitude and longitude. Then we can use the quotient of those divisions to identify our cells.\n",
    "As we have two dimensions, we find two sets of intervals ( one on latitude and one on longitude) with the origin of the gird on (0,0). Both sets can be either positive or negative to map all the four earth's dial. Thus, two \"check-ins\" with the same latitude and longitude intervals will be classified in the same cell. Note that to do a euclidean division by 25 km, we need to transform all latitudes and longitudes into kilometers and take into account the distortion of the earth. This is performed by two functions which follow the formula from this website :\n",
    "https://stackoverflow.com/questions/1253499/simple-calculations-for-working-with-lat-lon-and-km-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_degree_to_km(latitude):\n",
    "    '''\n",
    "    Input : Latitude in degree\n",
    "    Output : Latitude in km\n",
    "    \n",
    "    '''\n",
    "    return latitude * 110.574\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_degree_to_km(latitude,longitude):\n",
    "    '''\n",
    "    Input : Longitude , Latitude in degree\n",
    "    Output : Longitude in km  \n",
    "    \n",
    "    '''\n",
    "    return 111.320 * np.cos((latitude * np.pi) / 180) * longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classing_into_cells(g_checkin_df):\n",
    "    '''\n",
    "    Input : the time and location information of check-ins made by users\n",
    "    Output : check-in dataset classified by cells\n",
    "    \n",
    "    '''\n",
    "    # Creation of columns regrouping km for longitude and latitude with the two function see above\n",
    "    \n",
    "    g_checkin_df['lat_km'] = lat_degree_to_km(g_checkin_df['latitude']) \n",
    "    g_checkin_df['long_km'] = long_degree_to_km(g_checkin_df['latitude'],g_checkin_df['longitude'])\n",
    "    \n",
    "    # creation of cells with size 25 x 25 km represented by two sets of intervals int_longitude and int_latitude. \n",
    "    # By taking those two together we can build a grid with an origin at (0,0)\n",
    "    # Note that when the interval is transfering from float to int he takes the floor number for \n",
    "    # positive number and the ceil number for the negative interval, that's what we want .\n",
    "    \n",
    "    g_checkin_df['int_lat'] = (g_checkin_df['lat_km'] / 25).astype(int)\n",
    "    g_checkin_df['int_long'] = (g_checkin_df['long_km'] / 25).astype(int)\n",
    "    \n",
    "    return g_checkin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df = classing_into_cells(g_checkin_df)\n",
    "b_checkin_df = classing_into_cells(b_checkin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the location of the users (latitude, longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find the location of the user's houses. Thus, we assume that the position of the house of the user will be at the mean of all the check-ins positions (latitude and longitude) appearing in one particular cell.\n",
    "This particular cell is established by analyzing all cells linked to one user and pick the one that has the most number of check-ins in it. The article claims that this method is known to be accurate at 85 %.\n",
    "\n",
    "##### Method:\n",
    "Here, we use the function \"groupby\" to get a new dataset that grouped for each user all his check-ins by cells by counting how many rows appear to have the same \"int_lat\", \"int_long\" and \"user\". So then, we can sort all counts by users to take only the top one with the highest number of check-ins. We merge this dataset to the one classified by cells. Then, by comparing all check-ins cells and the previous result we can find the rows representing the cell with the most check-ins. Finally, the location of the house will be the average position of all the check-ins contained in this particular cell. \n",
    "\n",
    "Note that in the paper \"Friendship and Mobility: User Movement in Location-Based Social Networks\", any information was provided on how to deal with users that had the same number of check-ins on all his cells. For them, we decide to just take the cell located at the top of the user's groupby after sorting it without any further research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_user_house(g_checkin_df):\n",
    "    '''\n",
    "    Input : check-in dataset classified by cells\n",
    "    Output : dataset link each user's with his house location (latitude, longitude)\n",
    "    \n",
    "    '''\n",
    "    # finding intervals that has the highest number of check-ins in it\n",
    "    \n",
    "    int_house_users = g_checkin_df.groupby(['user','int_lat','int_long']).count()\n",
    "    int_house_users = int_house_users['checkin_time'].sort_values(\n",
    "        ascending = False).groupby(level = 0).head(1).reset_index()\n",
    "    int_house_users = int_house_users.rename(\n",
    "        {'int_lat' : 'house_int_latitude','int_long' : 'house_int_longitude'},\n",
    "        axis = \"columns\")\n",
    "    \n",
    "    # finding all the check-ins made in those intervals (cells)\n",
    "    \n",
    "    house_users = g_checkin_df.merge(int_house_users,\n",
    "                    left_on = ['user'],\n",
    "                    right_on = ['user'])\n",
    "    \n",
    "    house_users = house_users.loc[(house_users['int_lat'] == house_users['house_int_latitude'])\n",
    "                            & (house_users['int_long'] == house_users['house_int_longitude'])]\n",
    "    \n",
    "    #finding latitude and longitude of the user's house.\n",
    "    \n",
    "    houses = house_users.groupby(['user']).mean()[['latitude','longitude']].rename(\n",
    "        { 'latitude':'user_house_lat' ,'longitude':'user_house_long'},\n",
    "        axis = \"columns\")\n",
    "    return houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_gowalla = find_user_house(g_checkin_df)\n",
    "houses_bright = find_user_house(b_checkin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching the houses of the users and their friends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the result from the last section, we have at our disposal two data set which maps all users with the location of their houses. In addition to this, we also have the two Friendship undirected network initially provided. As stated in the introduction, our goal is to find the distance between each user and his friends. In order to do that we match the two networks (\"g_network_df\" and \"b_network_df\") with both the location of their user's houses and the location of the friend's houses in one dataset. Then, we can calculate the distance for each row by the following method of the sinus:\n",
    "http://villemin.gerard.free.fr/aGeograp/Distance.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_km(Lat_start,Long_start,Lat_end,Long_end):\n",
    "    '''\n",
    "    Input : latitude and longitude of a point A and B\n",
    "    Output : Distance between A and B\n",
    "    \n",
    "    '''\n",
    "    # Calcul with the method of sinus\n",
    "    distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
    "                          + np.cos(np.pi * Lat_start / 180.0) * np.cos(np.pi * Lat_end / 180.0) \\\n",
    "                            * np.cos(np.pi * Long_start / 180.0 - np.pi * Long_end / 180.0)) * 6371\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_user_friend(houses,network_df):\n",
    "    '''  \n",
    "    \n",
    "    Input: two dataset one containing the dataset which link each user's with his house location (latitude, longitude)\n",
    "    and one other containing the friendship network of users\n",
    "    Output: one dataset with the location of both users and their friends and with the distance linking the both houses\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # merge the location of the friends houses to the network data set\n",
    "    \n",
    "    friend_house_location = houses.reset_index().rename({'user':'friend_edge'},axis = 'columns')\n",
    "    friend_house_location = network_df.merge(friend_house_location,\n",
    "                                           on = 'friend_edge').rename(\n",
    "        {'user_house_lat' : 'friend_house_lat','user_house_long' : 'friend_house_long'},\n",
    "        axis = \"columns\")\n",
    "    \n",
    "    # merge the location of the users houses to this same dataset\n",
    "\n",
    "    network_houses_locations_df = friend_house_location.merge(houses, on ='user')\n",
    "    \n",
    "    # calculate the distance between each users and their friends\n",
    "    \n",
    "    network_houses_locations_df['dist_btw_user_friend'] = dist_km(network_houses_locations_df['user_house_lat'],\n",
    "                                                                  network_houses_locations_df['user_house_long']\n",
    "                                                              ,network_houses_locations_df['friend_house_lat'],\n",
    "                                                                  network_houses_locations_df['friend_house_long'])\n",
    "    \n",
    "    return network_houses_locations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_network_houses_locations_df = matching_user_friend(houses_gowalla, g_network_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_network_houses_locations_df = matching_user_friend(houses_bright,b_network_df)\n",
    "# Here we have an error for the rows that are combining a user and a friend that live in the same cell so\n",
    "# we are simply replacing the Nan values by zero to take those values into account.\n",
    "b_network_houses_locations_df['dist_btw_user_friend'].loc[b_network_houses_locations_df['dist_btw_user_friend'].isna()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_network_houses_locations_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_network_houses_locations_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding extra data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_gowalla = houses_gowalla.reset_index()\n",
    "houses_bright = houses_bright.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making geodataframe with the user home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_home = gpd.GeoDataFrame(\n",
    "    houses_gowalla, geometry=gpd.points_from_xy(houses_gowalla.user_house_long,houses_gowalla.user_house_lat))\n",
    "gdf_home_b = gpd.GeoDataFrame(\n",
    "    houses_bright, geometry=gpd.points_from_xy(houses_bright.user_house_long,houses_bright.user_house_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_home.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the shape file from https://www.igismap.com/united-states-shapefile-download-free-map-boundary-states-and-county/ called Download Polygon Shapefile of United States of America you can find the link for the download here https://map.igismap.com/share-map/export-layer/Alabama_AL4_US_Poly/1534b76d325a8f591b52d302e7181331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shp\n",
    "states =gpd.read_file(INP_FOLDER + 'United_States-_States_Polygon.shp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can join our user by states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting coordinate for sake of geopandas\n",
    "gdf_home.crs = \"EPSG:4326\"\n",
    "gdf_home_b.crs = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we append GeoDataFrame of users house to checkin dataframe\n",
    "def sjoin_chunk(chunk):\n",
    "    chunk.crs = \"EPSG:4326\"\n",
    "    return sjoin(chunk, states, how='left')\n",
    "\n",
    "def parallelize_dataframe_func(df, func):\n",
    "    num_cores = multiprocessing.cpu_count()-1  #leave one free to not freeze machine\n",
    "    num_partitions = num_cores #number of partitions to split dataframe\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "pointInStates = parallelize_dataframe_func(gdf_home, sjoin_chunk)\n",
    "pointInStates_b = parallelize_dataframe_func(gdf_home_b, sjoin_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear the user outside the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping data for users that reside in USA\n",
    "pointInStates = pointInStates.loc[pointInStates['country']=='USA']\n",
    "pointInStates_b = pointInStates_b.loc[pointInStates_b['country']=='USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning to have only what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointInStates = pointInStates[['user','user_house_lat','user_house_long','name']]\n",
    "pointInStates_b = pointInStates_b[['user','user_house_lat','user_house_long','name']]\n",
    "user_in_usa_g = pointInStates['user']\n",
    "user_in_usa_b = pointInStates_b['user']\n",
    "gdf_home_plot = gpd.GeoDataFrame(pointInStates, geometry=gpd.points_from_xy(pointInStates.user_house_long,pointInStates.user_house_lat))\n",
    "gdf_home_plot_b = gpd.GeoDataFrame(pointInStates_b, geometry=gpd.points_from_xy(pointInStates_b.user_house_long,pointInStates_b.user_house_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointInStates.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the major states\n",
    "# list of states (for visualisation)\n",
    "list_major_states =[\"Alabama\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "  \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n",
    "point_in_major_states = gdf_home_plot.loc[gdf_home_plot['name'].isin(list_major_states)==True]\n",
    "point_in_major_states_b= gdf_home_plot_b.loc[gdf_home_plot_b['name'].isin(list_major_states)==True]\n",
    "\n",
    "major_states =states.loc[states['name'].isin(list_major_states)==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the location of users' house over USA map; however we have prepared better results for the data story\n",
    "\n",
    "ax = major_states['geometry'].plot(cmap='Greys',figsize=(25, 20),edgecolor='k')\n",
    "point_in_major_states['geometry'].plot(ax=ax,color='r',markersize =4)\n",
    "point_in_major_states_b['geometry'].plot(ax=ax,color='b',markersize =4)\n",
    "plt.title('\\n Major states of U.S with the user home location \\n ',size=40)\n",
    "plt.xlabel('\\n Longitude \\n ',size=40)\n",
    "plt.ylabel('\\n Latitude \\n',size=40)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(['Home Gowalla user','Home Brightkite user'],fontsize=20,loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin = gpd.GeoDataFrame(\n",
    "    g_checkin_df, geometry=gpd.points_from_xy(g_checkin_df.longitude,g_checkin_df.latitude))\n",
    "gdf_checkin_b = gpd.GeoDataFrame(\n",
    "    b_checkin_df, geometry=gpd.points_from_xy(b_checkin_df.longitude,b_checkin_df.latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkin only in the usa made by usa user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin_usa_g = point_in_major_states.merge(gdf_checkin,left_on = ['user'],right_on = ['user'])\n",
    "gdf_checkin_usa_b = point_in_major_states_b.merge(gdf_checkin_b,left_on = ['user'],right_on = ['user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning and reordering data\n",
    "COLUMN_TO_DROP = ['lat_km','long_km','int_lat','int_long','geometry_y']\n",
    "gdf_checkin_usa_g = gdf_checkin_usa_g.drop(COLUMN_TO_DROP,axis=1)\n",
    "gdf_checkin_usa_b = gdf_checkin_usa_b.drop(COLUMN_TO_DROP,axis=1)\n",
    "gdf_checkin_usa_g = gdf_checkin_usa_g.rename({'name':'States of homes'\n",
    "                                             ,'geometry_x':'geometry homes'},axis=1)\n",
    "gdf_checkin_usa_b = gdf_checkin_usa_b.rename({'name':'States of homes'\n",
    "                                             ,'geometry_x':'geometry homes'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the geoframedata of checkin locations\n",
    "gdf_checkin_usa_g = gpd.GeoDataFrame(\n",
    "    gdf_checkin_usa_g, geometry=gpd.points_from_xy(gdf_checkin_usa_g.longitude,\n",
    "                                                   gdf_checkin_usa_g.latitude))\n",
    "gdf_checkin_usa_b = gpd.GeoDataFrame(\n",
    "    gdf_checkin_usa_b, geometry=gpd.points_from_xy(gdf_checkin_usa_b.longitude,\n",
    "                                                   gdf_checkin_usa_b.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending the geoframedata of checkin locations\n",
    "check_in_pointInStates_b = parallelize_dataframe_func(gdf_checkin_usa_b, sjoin_chunk)\n",
    "# reordering data\n",
    "check_in_pointInStates_b = check_in_pointInStates_b[['user','user_house_lat',\n",
    "                                                     'user_house_long','States of homes',\n",
    "                                                     'geometry homes',\n",
    "                                                     'checkin_time','latitude','longitude',\n",
    "                                                     'country','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "check_in_pointInStates_g = parallelize_dataframe_func(gdf_checkin_usa_g, sjoin_chunk)\n",
    "# reordering data\n",
    "check_in_pointInStates_g = check_in_pointInStates_g[['user','user_house_lat','user_house_long','States of homes','geometry homes',\n",
    "                                                     'checkin_time','latitude','longitude','country','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding time as datetime object to the dataframe\n",
    "check_in_pointInStates_b['date'] =\\\n",
    "check_in_pointInStates_b['checkin_time'].astype(str).parallel_apply(lambda x: np.datetime64(x.split(\"T\", 1)[0]))\n",
    "\n",
    "check_in_pointInStates_g['date'] =\\\n",
    "check_in_pointInStates_g['checkin_time'].astype(str).parallel_apply(lambda x: np.datetime64(x.split(\"T\", 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(check_in_pointInStates_g)=}\")\n",
    "print(f\"{len(check_in_pointInStates_b)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the prepared ghcn (Global Historical Climatology Network) data which are:\n",
    "# 1. minimum temperature\n",
    "# 2. maximum temperature\n",
    "# 3. precipitation\n",
    "# over a day\n",
    "ds_tmin = xr.open_dataset(\"./InputData/coarse_tmmn.nc\")\n",
    "ds_tmax = xr.open_dataset(\"./InputData/coarse_tmmx.nc\")\n",
    "ds_pr = xr.open_dataset(\"./InputData/coarse_pr.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following handful of cells we assign climatic data for each checkin for both users' house location and the checkin location with looking up from the grided data that we have in the ***xarray*** form the ***nc*** files loaded above. It works such that for every checkin we look up the climate data that we have for that date and we try to find the nearest grid point that we have to that location in our dataset and we take the value of the grid point that we have and assign it to our checkin.\n",
    "\n",
    "***Note***: we do not have climate data for the checkins outside of US an we will have some NaNs  in the climate data columns in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"tmin_house\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_tmin.sel(day=df.date, lon=df.user_house_long,\n",
    "                            lat=df.user_house_lat, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"tmax_house\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_tmax.sel(day=df.date, lon=df.user_house_long,\n",
    "                            lat=df.user_house_lat, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"pr_house\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_pr.sel(day=df.date, lon=df.user_house_long,\n",
    "                          lat=df.user_house_lat, method=\"nearest\",\n",
    "                          tolerance=400).get('precipitation_amount').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"tmin_checkin\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_tmin.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"tmax_checkin\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_tmax.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"pr_checkin\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_pr.sel(day=df.date, lon=df.longitude,\n",
    "                          lat=df.latitude, method=\"nearest\",\n",
    "                          tolerance=400).get('precipitation_amount').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(check_in_pointInStates_b)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"tmin_house\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_tmin.sel(day=df.date, lon=df.user_house_long,\n",
    "                            lat=df.user_house_lat, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"tmax_house\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_tmax.sel(day=df.date, lon=df.user_house_long,\n",
    "                            lat=df.user_house_lat, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"pr_house\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_pr.sel(day=df.date, lon=df.user_house_long,\n",
    "                            lat=df.user_house_lat, method=\"nearest\",\n",
    "                            tolerance=400).get('precipitation_amount').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"tmin_checkin\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_tmin.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"tmax_cehckin\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_tmax.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"pr_checkin\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_pr.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('precipitation_amount').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = holidays.UnitedStates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g['date'] = pd.to_datetime(check_in_pointInStates_g['date'], format = '%Y-%m-%d')\n",
    "check_in_pointInStates_b['date'] = pd.to_datetime(check_in_pointInStates_b['date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g['us_holiday'] = check_in_pointInStates_g['date'].parallel_apply(us_holidays.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b['us_holiday'] = check_in_pointInStates_b['date'].parallel_apply(us_holidays.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the dataframes with added data into disk\n",
    "check_in_pointInStates_g.to_csv('home_and_checkin_with_states_gowalla.csv')\n",
    "check_in_pointInStates_b.to_csv('home_and_checkin_with_states_bright.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics \n",
    "The metric that we use as evaluations of the checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy() or Load data to df_bright and df_gowalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get ready calculating our metrics:\n",
    "#df_df_brightt = check_in_pointInStates_b.copy()\n",
    "#df_gowalla = check_in_pointInStates_g.copy()\n",
    "df_bright = pd.read_csv('home_and_checkin_with_states_bright.csv')\n",
    "df_gowalla = pd.read_csv('home_and_checkin_with_states_gowalla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29881"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_bright['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49605"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_gowalla['user'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northern Hemisphere for local time of Denver, CO.\n",
    "https://www.calendardate.com/year2009.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons(date):\n",
    "    \n",
    "    spring_2008 = datetime.strptime('2008-03-19', '%Y-%m-%d')\n",
    "    summer_2008 = datetime.strptime('2008-06-20', '%Y-%m-%d')\n",
    "    autumn_2008 = datetime.strptime('2008-09-22', '%Y-%m-%d')\n",
    "    winter_2008 = datetime.strptime('2008-12-21', '%Y-%m-%d')\n",
    "    spring_2009 = datetime.strptime('2009-03-20', '%Y-%m-%d')\n",
    "    summer_2009 = datetime.strptime('2009-06-20', '%Y-%m-%d')\n",
    "    autumn_2009 = datetime.strptime('2009-09-22', '%Y-%m-%d')\n",
    "    winter_2009 = datetime.strptime('2009-12-21', '%Y-%m-%d')\n",
    "    spring_2010 = datetime.strptime('2010-03-20', '%Y-%m-%d')\n",
    "    summer_2010 = datetime.strptime('2010-06-21', '%Y-%m-%d')\n",
    "    autumn_2010 = datetime.strptime('2010-09-22', '%Y-%m-%d')\n",
    "    winter_2010 = datetime.strptime('2010-12-21', '%Y-%m-%d')\n",
    "    \n",
    "    if spring_2008 >= date:\n",
    "        return 'winter_2007'\n",
    "    elif summer_2008 >= date:\n",
    "        return 'spring_2008'\n",
    "    elif autumn_2008 >= date:\n",
    "        return 'summer_2008'\n",
    "    elif winter_2008 >= date:\n",
    "        return 'autumn_2008'\n",
    "    elif spring_2009 >= date:\n",
    "        return 'winter_2008'\n",
    "    elif summer_2009 >= date:\n",
    "        return 'spring_2009'\n",
    "    elif autumn_2009 >= date:\n",
    "        return 'summer_2009'\n",
    "    elif winter_2009 >= date:\n",
    "        return 'autumn_2009'\n",
    "    elif spring_2010 >= date:\n",
    "        return 'winter_2010'\n",
    "    elif summer_2010 >= date:\n",
    "        return 'spring_2010'\n",
    "    elif autumn_2010 >= date:\n",
    "        return 'summer_2010' \n",
    "    elif winter_2010 >= date:\n",
    "        return 'autumn_2010'\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla['season'] = df_gowalla['date'].parallel_apply(seasons)\n",
    "df_bright['season'] = df_bright['date'].parallel_apply(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance From Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n"
     ]
    }
   ],
   "source": [
    "df_gowalla['distance_from_home'] = df_gowalla.parallel_apply(lambda x: dist_km(x['user_house_lat'],\\\n",
    "                                                                     x['user_house_long'],\\\n",
    "                                                                     x['latitude'],\\\n",
    "                                                                     x['longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n"
     ]
    }
   ],
   "source": [
    "df_bright['distance_from_home'] = df_bright.parallel_apply(lambda x: dist_km(x['user_house_lat'],\\\n",
    "                                                                     x['user_house_long'],\\\n",
    "                                                                     x['latitude'],\\\n",
    "                                                                     x['longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between user check-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset(data):\n",
    "    data_reset = data.reset_index(drop=True)\n",
    "    data_offset = data_reset.set_index(np.arange(1, data_reset.shape[0]+1, 1))[['latitude','longitude']]\n",
    "    return data_reset.merge(data_offset, how='left',left_index=True, right_index=True,\\\n",
    "                            suffixes=['','_prev_check'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z stands for GMT and UTC timezone it is offset from 0 by coordinated time\n",
    "\n",
    "df_gowalla['time_obj'] = pd.to_datetime(df_gowalla['checkin_time'].str.replace('[T]',':').str.replace('Z',''),\\\n",
    "                                        format='%Y-%m-%d:%H:%M:%S')\n",
    "\n",
    "df_bright['time_obj'] = pd.to_datetime(df_bright['checkin_time'].str.replace('[T]',':').str.replace('Z',''),\\\n",
    "                                        format='%Y-%m-%d:%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>user_house_lat</th>\n",
       "      <th>user_house_long</th>\n",
       "      <th>States of homes</th>\n",
       "      <th>geometry homes</th>\n",
       "      <th>checkin_time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>pr</th>\n",
       "      <th>us_holiday</th>\n",
       "      <th>season</th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>time_obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-19T23:55:27Z</td>\n",
       "      <td>30.235909</td>\n",
       "      <td>-97.795140</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-19</td>\n",
       "      <td>288.600006</td>\n",
       "      <td>302.100006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>5.742689</td>\n",
       "      <td>2010-10-19 23:55:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-18T22:17:43Z</td>\n",
       "      <td>30.269103</td>\n",
       "      <td>-97.749395</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-18</td>\n",
       "      <td>289.100006</td>\n",
       "      <td>301.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>0.768984</td>\n",
       "      <td>2010-10-18 22:17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-17T23:42:03Z</td>\n",
       "      <td>30.255731</td>\n",
       "      <td>-97.763386</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-17</td>\n",
       "      <td>287.299988</td>\n",
       "      <td>301.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>1.999700</td>\n",
       "      <td>2010-10-17 23:42:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-17T19:26:05Z</td>\n",
       "      <td>30.263418</td>\n",
       "      <td>-97.757597</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-17</td>\n",
       "      <td>287.299988</td>\n",
       "      <td>301.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>1.245154</td>\n",
       "      <td>2010-10-17 19:26:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-16T18:50:42Z</td>\n",
       "      <td>30.274292</td>\n",
       "      <td>-97.740523</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>282.299988</td>\n",
       "      <td>301.299988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>1.258630</td>\n",
       "      <td>2010-10-16 18:50:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  user  user_house_lat  user_house_long States of homes  \\\n",
       "0           0     0       30.263544       -97.744633           Texas   \n",
       "1           1     0       30.263544       -97.744633           Texas   \n",
       "2           2     0       30.263544       -97.744633           Texas   \n",
       "3           3     0       30.263544       -97.744633           Texas   \n",
       "4           4     0       30.263544       -97.744633           Texas   \n",
       "\n",
       "                                 geometry homes          checkin_time  \\\n",
       "0  POINT (-97.74463265845054 30.26354372623895)  2010-10-19T23:55:27Z   \n",
       "1  POINT (-97.74463265845054 30.26354372623895)  2010-10-18T22:17:43Z   \n",
       "2  POINT (-97.74463265845054 30.26354372623895)  2010-10-17T23:42:03Z   \n",
       "3  POINT (-97.74463265845054 30.26354372623895)  2010-10-17T19:26:05Z   \n",
       "4  POINT (-97.74463265845054 30.26354372623895)  2010-10-16T18:50:42Z   \n",
       "\n",
       "    latitude  longitude country   name       date        tmin        tmax  \\\n",
       "0  30.235909 -97.795140     USA  Texas 2010-10-19  288.600006  302.100006   \n",
       "1  30.269103 -97.749395     USA  Texas 2010-10-18  289.100006  301.799988   \n",
       "2  30.255731 -97.763386     USA  Texas 2010-10-17  287.299988  301.799988   \n",
       "3  30.263418 -97.757597     USA  Texas 2010-10-17  287.299988  301.799988   \n",
       "4  30.274292 -97.740523     USA  Texas 2010-10-16  282.299988  301.299988   \n",
       "\n",
       "    pr us_holiday       season  distance_from_home            time_obj  \n",
       "0  0.0       None  autumn_2010            5.742689 2010-10-19 23:55:27  \n",
       "1  0.0       None  autumn_2010            0.768984 2010-10-18 22:17:43  \n",
       "2  0.0       None  autumn_2010            1.999700 2010-10-17 23:42:03  \n",
       "3  0.0       None  autumn_2010            1.245154 2010-10-17 19:26:05  \n",
       "4  0.0       None  autumn_2010            1.258630 2010-10-16 18:50:42  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gowalla.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 49605/49605 [03:07<00:00, 264.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df_gowalla_off = df_gowalla.sort_values('time_obj', ascending=True).groupby('user')\\\n",
    "                                                    .progress_apply(lambda group: offset(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 29881/29881 [01:52<00:00, 264.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df_bright_off = df_bright.sort_values('time_obj', ascending=True).groupby('user')\\\n",
    "                                                    .progress_apply(lambda group: offset(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off = df_gowalla_off.reset_index(drop=True)\n",
    "df_bright_off = df_bright_off.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n"
     ]
    }
   ],
   "source": [
    "df_gowalla_off['distance_from_prev_check'] = df_gowalla_off.parallel_apply(lambda x: dist_km(x['latitude'],\\\n",
    "                                                                     x['longitude'],\\\n",
    "                                                                     x['latitude_prev_check'],\\\n",
    "                                                                     x['longitude_prev_check']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-14-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n"
     ]
    }
   ],
   "source": [
    "df_bright_off['distance_from_prev_check'] = df_bright_off.parallel_apply(lambda x: dist_km(x['latitude'],\\\n",
    "                                                                     x['longitude'],\\\n",
    "                                                                     x['latitude_prev_check'],\\\n",
    "                                                                     x['longitude_prev_check']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Timezones & Local Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2812172/2812172 [00:29<00:00, 94626.68it/s] \n",
      "100%|| 3518110/3518110 [00:36<00:00, 95687.96it/s] \n"
     ]
    }
   ],
   "source": [
    "df_bright_off['timezone'] = df_bright_off[['longitude', 'latitude']].progress_apply(lambda x:tf.timezone_at(lng=x[0],lat=x[1]),axis=1)\n",
    "df_gowalla_off['timezone'] = df_gowalla_off[['longitude', 'latitude']].progress_apply(lambda x:tf.timezone_at(lng=x[0],lat=x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off['utc'] = df_gowalla_off['time_obj'].parallel_apply(timezone(utc.zone).localize)\n",
    "df_bright_off['utc'] = df_bright_off['time_obj'].parallel_apply(timezone(utc.zone).localize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_time_zone(row):\n",
    "    try:\n",
    "        return row[0].astimezone(timezone(row[1]))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off['local_time'] = df_gowalla_off[['utc','timezone']].parallel_apply(set_time_zone,axis=1)\n",
    "df_bright_off['local_time'] = df_bright_off[['utc','timezone']].parallel_apply(set_time_zone,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off['local_time_grouper'] = df_gowalla_off['local_time'].parallel_apply(lambda x: x.replace(tzinfo=None) if(pd.notnull(x)) else x )\n",
    "df_bright_off['local_time_grouper'] = df_bright_off['local_time'].parallel_apply(lambda x: x.replace(tzinfo=None) if(pd.notnull(x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off.to_csv('./Data/all_feat_gowalla.csv', index=False)\n",
    "df_bright_off.to_csv('./Data/all_feat_bright.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off[\"distance_from_prev_check\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_gowalla_off['distance_from_prev_check']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily Movement Entropy (Average entropy of check in locations over time)**\n",
    "- average shannon entropy of check in locations for each hour of the week\n",
    "- lower the entropy, lower the variability of check ins during that time period\n",
    "- need to recreate the graphs to make sure using correct shannon entropy\n",
    "\n",
    "H(X) = -$\\sum_{i=1}^{n} P(x_i)LogP(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took inspiration from this stack overflow for entropy\n",
    "https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Only take users who meet a certain criteria for number of check ins to avoid a bunch of 0 entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we include location ID use this instead of unique loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_entropy(data):\n",
    "    p_data = data.value_counts()\n",
    "    ent = stats.entropy(p_data, base=2)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright2 = df_bright.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright2['weekday'] = df_bright2['local_time_grouper'].map(lambda x: x.weekday())\n",
    "df_bright2['hour'] = df_bright2['local_time_grouper'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright2_group = df_bright2.groupby(['user','weekday','hour'])\n",
    "#df_bright_group = df_bright.groupby(pd.Grouper(key=\"local_time_grouper\", freq=\"1H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_bright2 = df_bright2_group['unique_loc'].parallel_apply(apply_entropy)\n",
    "entropy_bright2 = entropy_bright2.to_frame().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy_bright2 = entropy_bright2[entropy_bright2['unique_loc'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_mean_bright2 = entropy_bright2.drop('user', axis=1).groupby(['weekday','hour']).mean().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_mean_bright2['graph_date'] = entropy_mean_bright2.apply(lambda x: \\\n",
    "                                                          pd.Timestamp(year=2020,month=6,day=int(x['weekday']+1),\\\n",
    "                                                                       hour=int(x['hour'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax1.set(xlabel='', ylabel='Avg Entropy')\n",
    "ax1.plot(entropy_mean_bright2.graph_date, entropy_mean_bright2['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_week_hour.graph_date, entropy_week_hour['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_hour.index, df_bright.Registered, color='b')\n",
    "\n",
    "ax1.xaxis.set(\n",
    "    major_locator=mdates.DayLocator(),\n",
    "    major_formatter=mdates.DateFormatter(\"\\n\\n%A\"),\n",
    "    minor_locator=mdates.HourLocator((6 ,12, 18)),\n",
    "    minor_formatter=mdates.DateFormatter(\"%H\"),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_group = df_gowalla.groupby(pd.Grouper(key=\"local_time_grouper\", freq=\"1H\"))\n",
    "df_bright_group = df_bright.groupby(pd.Grouper(key=\"local_time_grouper\", freq=\"1H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_hour_gowalla = df_gowalla_group['unique_loc'].parallel_apply(apply_entropy)\n",
    "entropy_hour_gowalla = entropy_hour_gowalla.to_frame().reset_index(drop=False)\n",
    "\n",
    "entropy_hour_bright = df_bright_group['unique_loc'].parallel_apply(apply_entropy)\n",
    "entropy_hour_bright = entropy_hour_bright.to_frame().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_hour_gowalla['weekday'] = entropy_hour_gowalla['local_time_grouper'].map(lambda x: x.weekday())\n",
    "entropy_hour_gowalla['hour'] = entropy_hour_gowalla['local_time_grouper'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_hour_bright['weekday'] = entropy_hour_bright['local_time_grouper'].map(lambda x: x.weekday())\n",
    "entropy_hour_bright['hour'] = entropy_hour_bright['local_time_grouper'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_gowalla = entropy_hour_gowalla.groupby(['weekday','hour']).mean()\n",
    "entropy_week_hour_gowalla = entropy_week_hour_gowalla.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_bright = entropy_hour_bright.groupby(['weekday','hour']).mean()\n",
    "entropy_week_hour_bright = entropy_week_hour_bright.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_gowalla['graph_date'] = entropy_week_hour_gowalla.apply(lambda x: \\\n",
    "                                                          pd.Timestamp(year=2020,month=6,day=int(x['weekday']+1),\\\n",
    "                                                                       hour=int(x['hour'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_bright['graph_date'] = entropy_week_hour_bright.apply(lambda x: \\\n",
    "                                                          pd.Timestamp(year=2020,month=6,day=int(x['weekday']+1),\\\n",
    "                                                                       hour=int(x['hour'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax1.set(xlabel='', ylabel='Avg Entropy')\n",
    "ax1.plot(entropy_week_hour_gowalla.graph_date, entropy_week_hour_gowalla['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_week_hour.graph_date, entropy_week_hour['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_hour.index, df_bright.Registered, color='b')\n",
    "\n",
    "ax1.xaxis.set(\n",
    "    major_locator=mdates.DayLocator(),\n",
    "    major_formatter=mdates.DateFormatter(\"\\n\\n%A\"),\n",
    "    minor_locator=mdates.HourLocator((6 ,12, 18)),\n",
    "    minor_formatter=mdates.DateFormatter(\"%H\"),\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
