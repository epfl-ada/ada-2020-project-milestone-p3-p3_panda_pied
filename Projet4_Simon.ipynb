{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Republication report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we attempt replicating the figure 2.a from the article \"Friendship and Mobility: User Movement in Location-Based Social Networks \". The figure represents the relation between the probability of friendship and the distance between their houses. To replicate it, we will first find the position of the houses of each user by using the method described in the article and then assign a distance between each user and his friends. Finally, we will be able to calculate the probability of occurrence for all the distance and replicate the figure. More details are provided in the following sections. The article can be found at this address: https://drive.google.com/drive/folders/1QRoC6DAMoD_BxJ6KPMdijBRTDUYhoVfG?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 23 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # needed for plotting\n",
    "import geopandas as gpd # for Geo-location filtering\n",
    "from netCDF4 import Dataset as nc  # for loading standard climate date format (nc extension)\n",
    "import netCDF4 # for loading standard climate date format (nc extension)\n",
    "import xarray as xr # for handling climate data\n",
    "from pandarallel import pandarallel # for running pandas functions in parallel\n",
    "import multiprocessing # for general parallelizing of codes\n",
    "import tqdm # for having progres bar\n",
    "from functools import partial # for full control over handling function arguemnts\n",
    "pandarallel.initialize(nb_workers=multiprocessing.cpu_count()-1)\n",
    "from geopandas.tools import sjoin\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm # for having progres bar\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from scipy import stats\n",
    "import holidays\n",
    "from timezonefinder import TimezoneFinderL\n",
    "tf = TimezoneFinderL(in_memory=False)\n",
    "\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "utc = pytz.utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of two set of data\n",
    "DATA_FOLDER = 'Data/'\n",
    "\n",
    "INP_FOLDER = 'InputData/'\n",
    "\n",
    "# Friendship undirected network users and Time and location information of check-ins made by users\n",
    "\n",
    "#Brightkite\n",
    "BRIGHT_NETWORK_DATASET = DATA_FOLDER + \"Brightkite_edges.txt.gz\"\n",
    "BRIGHT_CHECKIN_TIME_LOCATION_DATASET = DATA_FOLDER + \"Brightkite_totalCheckins.txt.gz\"\n",
    "\n",
    "#Gowalla\n",
    "GOWALLA_NETWORK_DATASET = DATA_FOLDER + \"Gowalla_edges.txt.gz\"\n",
    "GOWALLA_CHECKIN_TIME_LOCATION_DATASET = DATA_FOLDER + \"Gowalla_totalCheckins.txt.gz\"\n",
    "\n",
    "# loading all the data set\n",
    "b_network_df = pd.read_csv(BRIGHT_NETWORK_DATASET, delimiter=\"\\t\",\n",
    "                                    error_bad_lines =False, header = 0 )\n",
    "b_checkin_df = pd.read_csv(BRIGHT_CHECKIN_TIME_LOCATION_DATASET, delimiter=\"\\t\",\n",
    "                                       error_bad_lines =False, header = 0 )\n",
    "g_network_df = pd.read_csv(GOWALLA_NETWORK_DATASET, delimiter = \"\\t\",header = None)\n",
    "g_checkin_df = pd.read_csv(GOWALLA_CHECKIN_TIME_LOCATION_DATASET, delimiter = \"\\t\",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the header \n",
    "NETWORK_COLUMNS = ['user','friend_edge']\n",
    "CHECKIN_COLUMNS = ['user','checkin_time','latitude','longitude','location_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns\n",
    "\n",
    "b_network_df.columns = NETWORK_COLUMNS\n",
    "b_checkin_df.columns = CHECKIN_COLUMNS\n",
    "g_network_df.columns = NETWORK_COLUMNS\n",
    "g_checkin_df.columns = CHECKIN_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_network_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_network_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, before starting our recollection, we need to pre-process our dataset. So, we can begin by counting all the null values and remove them if necessary. We also want the latitude and longitude to be respectively in the interval [-90,90] and [-180,180]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_checkin_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cleaning the data'''\n",
    "\n",
    "# locate rows with NaN\n",
    "rows_with_NaN_b = b_checkin_df.loc[(b_checkin_df['latitude'].isna() == True)]\n",
    "rows_with_NaN_g = g_checkin_df.loc[(g_checkin_df['latitude'].isna() == True)]\n",
    "\n",
    "# locate rows that are not in those interval : -90 < Latitude < 90 and -180 < Longitude < 180\n",
    "\n",
    "# latitude\n",
    "lat_to_removed_b = b_checkin_df.loc[(b_checkin_df['latitude'] > 90) | (b_checkin_df['latitude'] < -90)]\n",
    "lat_to_removed_g = g_checkin_df.loc[(g_checkin_df['latitude'] > 90) | (g_checkin_df['latitude'] < -90)]\n",
    "\n",
    "# longitude\n",
    "long_to_removed_b = b_checkin_df.loc[(b_checkin_df['longitude'] > 180) | (b_checkin_df['longitude'] < -180)]\n",
    "long_to_removed_g = g_checkin_df.loc[(g_checkin_df['longitude'] > 180) | (g_checkin_df['longitude'] < -180)]\n",
    "\n",
    "# Dropping all the rows\n",
    "\n",
    "# Brightkite\n",
    "b_checkin_df = b_checkin_df.drop(rows_with_NaN_b.index)\n",
    "b_checkin_df = b_checkin_df.drop(lat_to_removed_b.index)\n",
    "b_checkin_df = b_checkin_df.drop(long_to_removed_b.index)\n",
    "\n",
    "#see bellow\n",
    "b_checkin_df = b_checkin_df.drop(b_checkin_df[b_checkin_df['location_id'] == '00000000000000000000000000000000'].index)\n",
    "\n",
    "# Gowalla\n",
    "g_checkin_df = g_checkin_df.drop(rows_with_NaN_g.index)\n",
    "g_checkin_df = g_checkin_df.drop(lat_to_removed_g.index)\n",
    "g_checkin_df = g_checkin_df.drop(long_to_removed_g.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some weird values were spotted for the bright kite dataset on the position (0,0) with a \"location_id\" equal to \" 00000...0000 \". As quoted on page 1083 of our article: \" The total number of check-ins for Gowalla is 6.4 million and 4.5 million for Brightkite. \" So to be more consistent with those numbers we decide to also remove them. We can now show the shape of our two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of the check-ins used in the recollection \n",
    "print('The total number of check-ins for Gowalla :',g_checkin_df.shape[0])\n",
    "print('The total number of check-ins for Brightkite :',b_checkin_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Recollection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classing every check-ins in cells of 25x25 km \n",
    "In the next two sections, we will provide more details on the method described in the paper.\n",
    "So, we begin by classing every check-in in a grid of 25x25 km over the world. One way of classing into cells is to perform euclidean division by 25 on both latitude and longitude. Then we can use the quotient of those divisions to identify our cells.\n",
    "As we have two dimensions, we find two sets of intervals ( one on latitude and one on longitude) with the origin of the gird on (0,0). Both sets can be either positive or negative to map all the four earth's dial. Thus, two \"check-ins\" with the same latitude and longitude intervals will be classified in the same cell. Note that to do a euclidean division by 25 km, we need to transform all latitudes and longitudes into kilometers and take into account the distortion of the earth. This is performed by two functions which follow the formula from this website :\n",
    "https://stackoverflow.com/questions/1253499/simple-calculations-for-working-with-lat-lon-and-km-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_degree_to_km(latitude):\n",
    "    '''\n",
    "    Input : Latitude in degree\n",
    "    Output : Latitude in km\n",
    "    \n",
    "    '''\n",
    "    return latitude * 110.574\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_degree_to_km(latitude,longitude):\n",
    "    '''\n",
    "    Input : Longitude , Latitude in degree\n",
    "    Output : Longitude in km  \n",
    "    \n",
    "    '''\n",
    "    return 111.320 * np.cos((latitude * np.pi) / 180) * longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classing_into_cells(g_checkin_df):\n",
    "    '''\n",
    "    Input : the time and location information of check-ins made by users\n",
    "    Output : check-in dataset classified by cells\n",
    "    \n",
    "    '''\n",
    "    # Creation of columns regrouping km for longitude and latitude with the two function see above\n",
    "    \n",
    "    g_checkin_df['lat_km'] = lat_degree_to_km(g_checkin_df['latitude']) \n",
    "    g_checkin_df['long_km'] = long_degree_to_km(g_checkin_df['latitude'],g_checkin_df['longitude'])\n",
    "    \n",
    "    # creation of cells with size 25 x 25 km represented by two sets of intervals int_longitude and int_latitude. \n",
    "    # By taking those two together we can build a grid with an origin at (0,0)\n",
    "    # Note that when the interval is transfering from float to int he takes the floor number for \n",
    "    # positive number and the ceil number for the negative interval, that's what we want .\n",
    "    \n",
    "    g_checkin_df['int_lat'] = (g_checkin_df['lat_km'] / 25).astype(int)\n",
    "    g_checkin_df['int_long'] = (g_checkin_df['long_km'] / 25).astype(int)\n",
    "    \n",
    "    return g_checkin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df = classing_into_cells(g_checkin_df)\n",
    "b_checkin_df = classing_into_cells(b_checkin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_checkin_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the location of the users (latitude, longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find the location of the user's houses. Thus, we assume that the position of the house of the user will be at the mean of all the check-ins positions (latitude and longitude) appearing in one particular cell.\n",
    "This particular cell is established by analyzing all cells linked to one user and pick the one that has the most number of check-ins in it. The article claims that this method is known to be accurate at 85 %.\n",
    "\n",
    "##### Method:\n",
    "Here, we use the function \"groupby\" to get a new dataset that grouped for each user all his check-ins by cells by counting how many rows appear to have the same \"int_lat\", \"int_long\" and \"user\". So then, we can sort all counts by users to take only the top one with the highest number of check-ins. We merge this dataset to the one classified by cells. Then, by comparing all check-ins cells and the previous result we can find the rows representing the cell with the most check-ins. Finally, the location of the house will be the average position of all the check-ins contained in this particular cell. \n",
    "\n",
    "Note that in the paper \"Friendship and Mobility: User Movement in Location-Based Social Networks\", any information was provided on how to deal with users that had the same number of check-ins on all his cells. For them, we decide to just take the cell located at the top of the user's groupby after sorting it without any further research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_user_house(g_checkin_df):\n",
    "    '''\n",
    "    Input : check-in dataset classified by cells\n",
    "    Output : dataset link each user's with his house location (latitude, longitude)\n",
    "    \n",
    "    '''\n",
    "    # finding intervals that has the highest number of check-ins in it\n",
    "    \n",
    "    int_house_users = g_checkin_df.groupby(['user','int_lat','int_long']).count()\n",
    "    int_house_users = int_house_users['checkin_time'].sort_values(\n",
    "        ascending = False).groupby(level = 0).head(1).reset_index()\n",
    "    int_house_users = int_house_users.rename(\n",
    "        {'int_lat' : 'house_int_latitude','int_long' : 'house_int_longitude'},\n",
    "        axis = \"columns\")\n",
    "    \n",
    "    # finding all the check-ins made in those intervals (cells)\n",
    "    \n",
    "    house_users = g_checkin_df.merge(int_house_users,\n",
    "                    left_on = ['user'],\n",
    "                    right_on = ['user'])\n",
    "    \n",
    "    house_users = house_users.loc[(house_users['int_lat'] == house_users['house_int_latitude'])\n",
    "                            & (house_users['int_long'] == house_users['house_int_longitude'])]\n",
    "    \n",
    "    #finding latitude and longitude of the user's house.\n",
    "    \n",
    "    houses = house_users.groupby(['user']).mean()[['latitude','longitude']].rename(\n",
    "        { 'latitude':'user_house_lat' ,'longitude':'user_house_long'},\n",
    "        axis = \"columns\")\n",
    "    return houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_gowalla = find_user_house(g_checkin_df)\n",
    "houses_bright = find_user_house(b_checkin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching the houses of the users and their friends "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the result from the last section, we have at our disposal two data set which maps all users with the location of their houses. In addition to this, we also have the two Friendship undirected network initially provided. As stated in the introduction, our goal is to find the distance between each user and his friends. In order to do that we match the two networks (\"g_network_df\" and \"b_network_df\") with both the location of their user's houses and the location of the friend's houses in one dataset. Then, we can calculate the distance for each row by the following method of the sinus:\n",
    "http://villemin.gerard.free.fr/aGeograp/Distance.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_km(Lat_start,Long_start,Lat_end,Long_end):\n",
    "    '''\n",
    "    Input : latitude and longitude of a point A and B\n",
    "    Output : Distance between A and B\n",
    "    \n",
    "    '''\n",
    "    # Calcul with the method of sinus\n",
    "    distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
    "                          + np.cos(np.pi * Lat_start / 180.0) * np.cos(np.pi * Lat_end / 180.0) \\\n",
    "                            * np.cos(np.pi * Long_start / 180.0 - np.pi * Long_end / 180.0)) * 6371\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_user_friend(houses,network_df):\n",
    "    '''  \n",
    "    \n",
    "    Input: two dataset one containing the dataset which link each user's with his house location (latitude, longitude)\n",
    "    and one other containing the friendship network of users\n",
    "    Output: one dataset with the location of both users and their friends and with the distance linking the both houses\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # merge the location of the friends houses to the network data set\n",
    "    \n",
    "    friend_house_location = houses.reset_index().rename({'user':'friend_edge'},axis = 'columns')\n",
    "    friend_house_location = network_df.merge(friend_house_location,\n",
    "                                           on = 'friend_edge').rename(\n",
    "        {'user_house_lat' : 'friend_house_lat','user_house_long' : 'friend_house_long'},\n",
    "        axis = \"columns\")\n",
    "    \n",
    "    # merge the location of the users houses to this same dataset\n",
    "\n",
    "    network_houses_locations_df = friend_house_location.merge(houses, on ='user')\n",
    "    \n",
    "    # calculate the distance between each users and their friends\n",
    "    \n",
    "    network_houses_locations_df['dist_btw_user_friend'] = dist_km(network_houses_locations_df['user_house_lat'],\n",
    "                                                                  network_houses_locations_df['user_house_long']\n",
    "                                                              ,network_houses_locations_df['friend_house_lat'],\n",
    "                                                                  network_houses_locations_df['friend_house_long'])\n",
    "    \n",
    "    return network_houses_locations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_network_houses_locations_df = matching_user_friend(houses_gowalla, g_network_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_network_houses_locations_df = matching_user_friend(houses_bright,b_network_df) \n",
    "# Here we have an error for the rows that are combining a user and a friend that live in the same cell so\n",
    "# we are simply replacing the Nan values by zero to take those values into account.\n",
    "b_network_houses_locations_df['dist_btw_user_friend'].loc[b_network_houses_locations_df['dist_btw_user_friend'].isna()] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_network_houses_locations_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_network_houses_locations_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we attempt to plot a figure similar to the one from the paper. At first, we rounded each distance over to the closest kilometer, but the obtained result was showing a lot of noises for high distance. So, to get rid of this issue, we decide to round all distance over a logarithmic scale. The rounding and probability are calculated through the NumPy function \"np.histogram\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logarithmic scale for rounding\n",
    "bins = np.logspace(0,5,75,endpoint = True)\n",
    "\n",
    "# rounding and calcul of the probability\n",
    "proba_g, bins = np.histogram(g_network_houses_locations_df['dist_btw_user_friend'].values, bins = bins ,density = True)\n",
    "proba_b, bins = np.histogram(b_network_houses_locations_df['dist_btw_user_friend'].values, bins = bins ,density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the result\n",
    "\n",
    "plt.figure(figsize = (9,7), facecolor = 'none')\n",
    "plt.loglog(bins[:-1], proba_b,marker = \"o\",color ='none',linewidth = 0,markersize = 13,markeredgecolor = \"b\")\n",
    "plt.loglog(bins[:-1], proba_g,marker = \"*\",color ='r',linewidth = 0,markersize = 13 ,markeredgecolor = \"r\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim((1,1e5))\n",
    "plt.ylim((1e-6,1))\n",
    "plt.legend(['Brightkite','Gowalla'],fontsize = 16)\n",
    "plt.xlabel('Distance between homes',fontsize = 12)\n",
    "plt.ylabel('Probability',fontsize = 14)\n",
    "plt.xticks(np.logspace(0,5,6),fontsize = 14)\n",
    "plt.yticks(np.logspace(-6,0,4),fontsize = 14)\n",
    "plt.title('(a) Friends',fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, a similar figure was found with some small differences, but those can depend a lot on which rounding we took to calculate our probability. So a perfect replication will be difficult to find because the rounding used in the paper was not provided. Note that those discrepancies could also be caused by the differences in the formula that can be used to calculate the distance between two points on the Earth (Methods of the sinus, Pythagore, or the Haversine). Nevertheless, we still have similar curves and the same proportion on both axes. We can also see a similar kink between the distance $10^2$ [km] and $10^3$ [km] in both figures and around $10^4$ [km] we have the same nonlinear repartition of probability.But, above all the paper's hypothesis that two people living very far away from each other are unlikely to be friends is verified and we have two independent datasets (Gowalla and Brightkite) that give the same result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading new data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_gowalla = houses_gowalla.reset_index()\n",
    "houses_bright = houses_bright.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making geodataframe with the user home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_home = gpd.GeoDataFrame(\n",
    "    houses_gowalla, geometry=gpd.points_from_xy(houses_gowalla.user_house_long,houses_gowalla.user_house_lat))\n",
    "gdf_home_b = gpd.GeoDataFrame(\n",
    "    houses_bright, geometry=gpd.points_from_xy(houses_bright.user_house_long,houses_bright.user_house_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_home.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the shape file from https://www.igismap.com/united-states-shapefile-download-free-map-boundary-states-and-county/ called Download Polygon Shapefile of United States of America you can find the link for the download here https://map.igismap.com/share-map/export-layer/Alabama_AL4_US_Poly/1534b76d325a8f591b52d302e7181331"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shp\n",
    "states =gpd.read_file(INP_FOLDER + 'United_States-_States_Polygon.shp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can join our user by states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdf_home.crs = \"EPSG:4326\"\n",
    "gdf_home_b.crs = \"EPSG:4326\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sjoin_chunk(chunk):\n",
    "    chunk.crs = \"EPSG:4326\"\n",
    "    return sjoin(chunk, states, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe_func(df, func):\n",
    "    num_cores = multiprocessing.cpu_count()-1  #leave one free to not freeze machine\n",
    "    num_partitions = num_cores #number of partitions to split dataframe\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointInStates = parallelize_dataframe_func(gdf_home, sjoin_chunk)\n",
    "pointInStates_b = parallelize_dataframe_func(gdf_home_b, sjoin_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pointInStates= sjoin(gdf_home, states, how='left')\n",
    "#pointInStates_b = sjoin(gdf_home_b,states, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_home.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear the user outside the USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointInStates = pointInStates.loc[pointInStates['country']=='USA']\n",
    "pointInStates_b = pointInStates_b.loc[pointInStates_b['country']=='USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning to have only what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointInStates = pointInStates[['user','user_house_lat','user_house_long','name']]\n",
    "pointInStates_b = pointInStates_b[['user','user_house_lat','user_house_long','name']]\n",
    "user_in_usa_g = pointInStates['user']\n",
    "user_in_usa_b = pointInStates_b['user']\n",
    "gdf_home_plot = gpd.GeoDataFrame(pointInStates, geometry=gpd.points_from_xy(pointInStates.user_house_long,pointInStates.user_house_lat))\n",
    "gdf_home_plot_b = gpd.GeoDataFrame(pointInStates_b, geometry=gpd.points_from_xy(pointInStates_b.user_house_long,pointInStates_b.user_house_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointInStates.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking only the major states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_major_states =[\"Alabama\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\n",
    "  \"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\"Idaho\",\"Illinois\",\n",
    "  \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
    "  \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\n",
    "  \"Nebraska\",\"Nevada\",\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\n",
    "  \"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\n",
    "  \"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
    "  \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_in_major_states = gdf_home_plot.loc[gdf_home_plot['name'].isin(list_major_states)==True]\n",
    "point_in_major_states_b= gdf_home_plot_b.loc[gdf_home_plot_b['name'].isin(list_major_states)==True]\n",
    "\n",
    "\n",
    "major_states =states.loc[states['name'].isin(list_major_states)==True]\n",
    "point_in_major_states['geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a graph of the user on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = major_states['geometry'].plot(cmap='Greys',figsize=(25, 20),edgecolor='k')\n",
    "point_in_major_states['geometry'].plot(ax=ax,color='r',markersize =4)\n",
    "point_in_major_states_b['geometry'].plot(ax=ax,color='b',markersize =4)\n",
    "plt.title('\\n Major states of U.S with the user home location \\n ',size=40)\n",
    "plt.xlabel('\\n Longitude \\n ',size=40)\n",
    "plt.ylabel('\\n Latitude \\n',size=40)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.legend(['Home Gowalla user','Home Brightkite user'],fontsize=20,loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_in_major_states.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_in_major_states_b.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin = gpd.GeoDataFrame(\n",
    "    g_checkin_df, geometry=gpd.points_from_xy(g_checkin_df.longitude,g_checkin_df.latitude))\n",
    "gdf_checkin_b = gpd.GeoDataFrame(\n",
    "    b_checkin_df, geometry=gpd.points_from_xy(b_checkin_df.longitude,b_checkin_df.latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkin only in the usa made by usa user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin_usa_g = point_in_major_states.merge(gdf_checkin,left_on = ['user'],right_on = ['user'])\n",
    "gdf_checkin_usa_b = point_in_major_states_b.merge(gdf_checkin_b,left_on = ['user'],right_on = ['user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMN_TO_DROP = ['location_id','lat_km','long_km','int_lat','int_long','geometry_y']\n",
    "COLUMN_TO_DROP = ['lat_km','long_km','int_lat','int_long','geometry_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin_usa_g = gdf_checkin_usa_g.drop(COLUMN_TO_DROP,axis=1)\n",
    "gdf_checkin_usa_b = gdf_checkin_usa_b.drop(COLUMN_TO_DROP,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin_usa_g = gdf_checkin_usa_g.rename({'name':'States of homes'\n",
    "                                             ,'geometry_x':'geometry homes'},axis=1)\n",
    "gdf_checkin_usa_b = gdf_checkin_usa_b.rename({'name':'States of homes'\n",
    "                                             ,'geometry_x':'geometry homes'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_checkin_usa_g = gpd.GeoDataFrame(\n",
    "    gdf_checkin_usa_g, geometry=gpd.points_from_xy(gdf_checkin_usa_g.longitude,gdf_checkin_usa_g.latitude))\n",
    "gdf_checkin_usa_b = gpd.GeoDataFrame(\n",
    "    gdf_checkin_usa_b, geometry=gpd.points_from_xy(gdf_checkin_usa_b.longitude,gdf_checkin_usa_b.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopandas.tools import sjoin\n",
    "#check_in_pointInStates_b = sjoin(gdf_checkin_usa_b,states, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b = parallelize_dataframe_func(gdf_checkin_usa_b, sjoin_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b = check_in_pointInStates_b[['user','user_house_lat','user_house_long','States of homes','geometry homes',\n",
    "                                                     'checkin_time','latitude','longitude','country','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_in_pointInStates_g = sjoin(gdf_checkin_usa_g, sudo rpm-ostree install opensslstates, how='left')\n",
    "check_in_pointInStates_g = parallelize_dataframe_func(gdf_checkin_usa_g, sjoin_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g = check_in_pointInStates_g[['user','user_house_lat','user_house_long','States of homes','geometry homes',\n",
    "                                                     'checkin_time','latitude','longitude','country','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b['date'] =\\\n",
    "check_in_pointInStates_b['checkin_time'].astype(str).parallel_apply(lambda x: np.datetime64(x.split(\"T\", 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g['date'] =\\\n",
    "check_in_pointInStates_g['checkin_time'].astype(str).parallel_apply(lambda x: np.datetime64(x.split(\"T\", 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tmin = xr.open_dataset(\"./InputData/coarse_tmmn.nc\")\n",
    "ds_tmax = xr.open_dataset(\"./InputData/coarse_tmmx.nc\")\n",
    "ds_pr = xr.open_dataset(\"./InputData/coarse_pr.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"tmin\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_tmin.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"tmax\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_tmax.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b[\"pr\"] =\\\n",
    "check_in_pointInStates_b.parallel_apply(\n",
    "    lambda df : ds_pr.sel(day=df.date, lon=df.longitude,\n",
    "                          lat=df.latitude, method=\"nearest\",\n",
    "                          tolerance=400).get('precipitation_amount').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"tmin\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_tmin.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"tmax\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_tmax.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('air_temperature').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g[\"pr\"] =\\\n",
    "check_in_pointInStates_g.parallel_apply(\n",
    "    lambda df : ds_pr.sel(day=df.date, lon=df.longitude,\n",
    "                            lat=df.latitude, method=\"nearest\",\n",
    "                            tolerance=400).get('precipitation_amount').data.ravel()[0],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_g.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_in_pointInStates_b.dropna(inplace=True)\n",
    "#check_in_pointInStates_g.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_in_pointInStates_b.to_csv('home_and_checkin_with_states_bright.csv')\n",
    "check_in_pointInStates_g.to_csv('home_and_checkin_with_states_gowalla.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy() or Load data to df_bright and df_gowalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_df_brightt = check_in_pointInStates_b.copy()\n",
    "#df_gowalla = check_in_pointInStates_g.copy()\n",
    "df_bright = pd.read_csv('home_and_checkin_with_states_bright.csv')\n",
    "df_gowalla = pd.read_csv('home_and_checkin_with_states_gowalla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1726"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_bright['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49605"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_gowalla['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright=df_bright.iloc[0:50,:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays = holidays.UnitedStates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla['date'] = pd.to_datetime(df_gowalla['date'], format = '%Y-%m-%d')\n",
    "df_bright['date'] = pd.to_datetime(df_bright['date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla['us_holiday'] = df_gowalla['date'].parallel_apply(us_holidays.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright['us_holiday'] = df_bright['date'].parallel_apply(us_holidays.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Northern Hemisphere for local time of Denver, CO.\n",
    "https://www.calendardate.com/year2009.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons(date):\n",
    "    \n",
    "    spring_2008 = datetime.strptime('2008-03-19', '%Y-%m-%d')\n",
    "    summer_2008 = datetime.strptime('2008-06-20', '%Y-%m-%d')\n",
    "    autumn_2008 = datetime.strptime('2008-09-22', '%Y-%m-%d')\n",
    "    winter_2008 = datetime.strptime('2008-12-21', '%Y-%m-%d')\n",
    "    spring_2009 = datetime.strptime('2009-03-20', '%Y-%m-%d')\n",
    "    summer_2009 = datetime.strptime('2009-06-20', '%Y-%m-%d')\n",
    "    autumn_2009 = datetime.strptime('2009-09-22', '%Y-%m-%d')\n",
    "    winter_2009 = datetime.strptime('2009-12-21', '%Y-%m-%d')\n",
    "    spring_2010 = datetime.strptime('2010-03-20', '%Y-%m-%d')\n",
    "    summer_2010 = datetime.strptime('2010-06-21', '%Y-%m-%d')\n",
    "    autumn_2010 = datetime.strptime('2010-09-22', '%Y-%m-%d')\n",
    "    winter_2010 = datetime.strptime('2010-12-21', '%Y-%m-%d')\n",
    "    \n",
    "    if spring_2008 >= date:\n",
    "        return 'winter_2007'\n",
    "    elif summer_2008 >= date:\n",
    "        return 'spring_2008'\n",
    "    elif autumn_2008 >= date:\n",
    "        return 'summer_2008'\n",
    "    elif winter_2008 >= date:\n",
    "        return 'autumn_2008'\n",
    "    elif spring_2009 >= date:\n",
    "        return 'winter_2008'\n",
    "    elif summer_2009 >= date:\n",
    "        return 'spring_2009'\n",
    "    elif autumn_2009 >= date:\n",
    "        return 'summer_2009'\n",
    "    elif winter_2009 >= date:\n",
    "        return 'autumn_2009'\n",
    "    elif spring_2010 >= date:\n",
    "        return 'winter_2010'\n",
    "    elif summer_2010 >= date:\n",
    "        return 'spring_2010'\n",
    "    elif autumn_2010 >= date:\n",
    "        return 'summer_2010' \n",
    "    elif winter_2010 >= date:\n",
    "        return 'autumn_2010'\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla['season'] = df_gowalla['date'].parallel_apply(seasons)\n",
    "df_bright['season'] = df_bright['date'].parallel_apply(seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance From Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n"
     ]
    }
   ],
   "source": [
    "df_gowalla['distance_from_home'] = df_gowalla.parallel_apply(lambda x: dist_km(x['user_house_lat'],\\\n",
    "                                                                     x['user_house_long'],\\\n",
    "                                                                     x['latitude'],\\\n",
    "                                                                     x['longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright['distance_from_home'] = df_bright.parallel_apply(lambda x: dist_km(x['user_house_lat'],\\\n",
    "                                                                     x['user_house_long'],\\\n",
    "                                                                     x['latitude'],\\\n",
    "                                                                     x['longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between user check-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset(data):\n",
    "    data_reset = data.reset_index(drop=True)\n",
    "    data_offset = data_reset.set_index(np.arange(1, data_reset.shape[0]+1, 1))[['latitude','longitude']]\n",
    "    return data_reset.merge(data_offset, how='left',left_index=True, right_index=True,\\\n",
    "                            suffixes=['','_prev_check'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z stands for GMT and UTC timezone it is offset from 0 by coordinated time\n",
    "\n",
    "df_gowalla['time_obj'] = pd.to_datetime(df_gowalla['checkin_time'].str.replace('[T]',':').str.replace('Z',''),\\\n",
    "                                        format='%Y-%m-%d:%H:%M:%S')\n",
    "\n",
    "df_bright['time_obj'] = pd.to_datetime(df_bright['checkin_time'].str.replace('[T]',':').str.replace('Z',''),\\\n",
    "                                        format='%Y-%m-%d:%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>user_house_lat</th>\n",
       "      <th>user_house_long</th>\n",
       "      <th>States of homes</th>\n",
       "      <th>geometry homes</th>\n",
       "      <th>checkin_time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>pr</th>\n",
       "      <th>us_holiday</th>\n",
       "      <th>season</th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>time_obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-19T23:55:27Z</td>\n",
       "      <td>30.235909</td>\n",
       "      <td>-97.795140</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-19</td>\n",
       "      <td>288.600006</td>\n",
       "      <td>302.100006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>5.742689</td>\n",
       "      <td>2010-10-19 23:55:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-18T22:17:43Z</td>\n",
       "      <td>30.269103</td>\n",
       "      <td>-97.749395</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-18</td>\n",
       "      <td>289.100006</td>\n",
       "      <td>301.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>0.768984</td>\n",
       "      <td>2010-10-18 22:17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-17T23:42:03Z</td>\n",
       "      <td>30.255731</td>\n",
       "      <td>-97.763386</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-17</td>\n",
       "      <td>287.299988</td>\n",
       "      <td>301.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>1.999700</td>\n",
       "      <td>2010-10-17 23:42:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-17T19:26:05Z</td>\n",
       "      <td>30.263418</td>\n",
       "      <td>-97.757597</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-17</td>\n",
       "      <td>287.299988</td>\n",
       "      <td>301.799988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>1.245154</td>\n",
       "      <td>2010-10-17 19:26:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.263544</td>\n",
       "      <td>-97.744633</td>\n",
       "      <td>Texas</td>\n",
       "      <td>POINT (-97.74463265845054 30.26354372623895)</td>\n",
       "      <td>2010-10-16T18:50:42Z</td>\n",
       "      <td>30.274292</td>\n",
       "      <td>-97.740523</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>282.299988</td>\n",
       "      <td>301.299988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>autumn_2010</td>\n",
       "      <td>1.258630</td>\n",
       "      <td>2010-10-16 18:50:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  user  user_house_lat  user_house_long States of homes  \\\n",
       "0           0     0       30.263544       -97.744633           Texas   \n",
       "1           1     0       30.263544       -97.744633           Texas   \n",
       "2           2     0       30.263544       -97.744633           Texas   \n",
       "3           3     0       30.263544       -97.744633           Texas   \n",
       "4           4     0       30.263544       -97.744633           Texas   \n",
       "\n",
       "                                 geometry homes          checkin_time  \\\n",
       "0  POINT (-97.74463265845054 30.26354372623895)  2010-10-19T23:55:27Z   \n",
       "1  POINT (-97.74463265845054 30.26354372623895)  2010-10-18T22:17:43Z   \n",
       "2  POINT (-97.74463265845054 30.26354372623895)  2010-10-17T23:42:03Z   \n",
       "3  POINT (-97.74463265845054 30.26354372623895)  2010-10-17T19:26:05Z   \n",
       "4  POINT (-97.74463265845054 30.26354372623895)  2010-10-16T18:50:42Z   \n",
       "\n",
       "    latitude  longitude country   name       date        tmin        tmax  \\\n",
       "0  30.235909 -97.795140     USA  Texas 2010-10-19  288.600006  302.100006   \n",
       "1  30.269103 -97.749395     USA  Texas 2010-10-18  289.100006  301.799988   \n",
       "2  30.255731 -97.763386     USA  Texas 2010-10-17  287.299988  301.799988   \n",
       "3  30.263418 -97.757597     USA  Texas 2010-10-17  287.299988  301.799988   \n",
       "4  30.274292 -97.740523     USA  Texas 2010-10-16  282.299988  301.299988   \n",
       "\n",
       "    pr us_holiday       season  distance_from_home            time_obj  \n",
       "0  0.0       None  autumn_2010            5.742689 2010-10-19 23:55:27  \n",
       "1  0.0       None  autumn_2010            0.768984 2010-10-18 22:17:43  \n",
       "2  0.0       None  autumn_2010            1.999700 2010-10-17 23:42:03  \n",
       "3  0.0       None  autumn_2010            1.245154 2010-10-17 19:26:05  \n",
       "4  0.0       None  autumn_2010            1.258630 2010-10-16 18:50:42  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gowalla.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49605/49605 [03:16<00:00, 252.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_gowalla_off = df_gowalla.sort_values('time_obj', ascending=True).groupby('user')\\\n",
    "                                                    .progress_apply(lambda group: offset(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 97.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df_bright_off = df_bright.sort_values('time_obj', ascending=True).groupby('user')\\\n",
    "                                                    .progress_apply(lambda group: offset(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off = df_gowalla_off.reset_index(drop=True)\n",
    "df_bright_off = df_bright_off.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n",
      "<ipython-input-55-1aab94a3e3b8>:8: RuntimeWarning: invalid value encountered in arccos\n",
      "  distance_km = np.arccos(np.sin(np.pi * Lat_start / 180.0) * np.sin(np.pi * Lat_end / 180.0) \\\n"
     ]
    }
   ],
   "source": [
    "df_gowalla_off['distance_from_prev_check'] = df_gowalla_off.parallel_apply(lambda x: dist_km(x['latitude'],\\\n",
    "                                                                     x['longitude'],\\\n",
    "                                                                     x['latitude_prev_check'],\\\n",
    "                                                                     x['longitude_prev_check']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright_off['distance_from_prev_check'] = df_bright_off.parallel_apply(lambda x: dist_km(x['latitude'],\\\n",
    "                                                                     x['longitude'],\\\n",
    "                                                                     x['latitude_prev_check'],\\\n",
    "                                                                     x['longitude_prev_check']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Timezones & Local Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 4233.93it/s]\n",
      "100%|██████████| 3518110/3518110 [00:38<00:00, 90664.07it/s] \n"
     ]
    }
   ],
   "source": [
    "df_bright_off['timezone'] = df_bright_off[['longitude', 'latitude']].progress_apply(lambda x:tf.timezone_at(lng=x[0],lat=x[1]),axis=1)\n",
    "df_gowalla_off['timezone'] = df_gowalla_off[['longitude', 'latitude']].progress_apply(lambda x:tf.timezone_at(lng=x[0],lat=x[1]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3518110/3518110 [00:26<00:00, 132023.80it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 38735.72it/s]\n"
     ]
    }
   ],
   "source": [
    "df_gowalla_off['utc'] = df_gowalla_off['time_obj'].progress_apply(timezone(utc.zone).localize)\n",
    "df_bright_off['utc'] = df_bright_off['time_obj'].progress_apply(timezone(utc.zone).localize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_time_zone(row):\n",
    "    try:\n",
    "        return row[0].astimezone(timezone(row[1]))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off['local_time'] = df_gowalla_off[['utc','timezone']].parallel_apply(set_time_zone,axis=1)\n",
    "df_bright_off['local_time'] = df_bright_off[['utc','timezone']].parallel_apply(set_time_zone,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3518110/3518110 [00:28<00:00, 124650.79it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 20584.53it/s]\n"
     ]
    }
   ],
   "source": [
    "df_gowalla_off['local_time_grouper'] = df_gowalla_off['local_time'].progress_apply(lambda x: x.replace(tzinfo=None) if(pd.notnull(x)) else x )\n",
    "df_bright_off['local_time_grouper'] = df_bright_off['local_time'].progress_apply(lambda x: x.replace(tzinfo=None) if(pd.notnull(x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_off.to_csv('./Data/all_feat_gowalla.csv', index=False)\n",
    "#df_bright_off.to_csv('./Data/all_feat_bright.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  NaN\n",
       "1          1043.413827\n",
       "2            35.990638\n",
       "3            12.091107\n",
       "4             0.067434\n",
       "              ...     \n",
       "3518105       0.546353\n",
       "3518106       0.537169\n",
       "3518107       2.521126\n",
       "3518108       2.128151\n",
       "3518109       6.041877\n",
       "Name: distance_from_prev_check, Length: 3518110, dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gowalla_off[\"distance_from_prev_check\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  user  user_house_lat  user_house_long States of homes  \\\n",
      "0         224     0       30.263544       -97.744633           Texas   \n",
      "1         223     0       30.263544       -97.744633           Texas   \n",
      "2         222     0       30.263544       -97.744633           Texas   \n",
      "3         221     0       30.263544       -97.744633           Texas   \n",
      "4         220     0       30.263544       -97.744633           Texas   \n",
      "\n",
      "                                 geometry homes          checkin_time  \\\n",
      "0  POINT (-97.74463265845054 30.26354372623895)  2010-05-22T02:49:04Z   \n",
      "1  POINT (-97.74463265845054 30.26354372623895)  2010-05-22T17:50:55Z   \n",
      "2  POINT (-97.74463265845054 30.26354372623895)  2010-05-22T19:13:12Z   \n",
      "3  POINT (-97.74463265845054 30.26354372623895)  2010-05-23T16:50:50Z   \n",
      "4  POINT (-97.74463265845054 30.26354372623895)  2010-05-23T17:51:57Z   \n",
      "\n",
      "    latitude  longitude country  ...       season distance_from_home  \\\n",
      "0  30.248924 -97.749626     USA  ...  spring_2010           1.694928   \n",
      "1  39.297443 -94.716053     USA  ...  spring_2010        1041.719821   \n",
      "2  38.985246 -94.605919     USA  ...  spring_2010        1011.245702   \n",
      "3  39.093533 -94.593174     USA  ...  spring_2010        1023.067327   \n",
      "4  39.093258 -94.593871     USA  ...  spring_2010        1023.020297   \n",
      "\n",
      "             time_obj  latitude_prev_check  longitude_prev_check  \\\n",
      "0 2010-05-22 02:49:04                  NaN                   NaN   \n",
      "1 2010-05-22 17:50:55            30.248924            -97.749626   \n",
      "2 2010-05-22 19:13:12            39.297443            -94.716053   \n",
      "3 2010-05-23 16:50:50            38.985246            -94.605919   \n",
      "4 2010-05-23 17:51:57            39.093533            -94.593174   \n",
      "\n",
      "  distance_from_prev_check         timezone                       utc  \\\n",
      "0                      NaN  America/Chicago 2010-05-22 02:49:04+00:00   \n",
      "1              1043.413827  America/Chicago 2010-05-22 17:50:55+00:00   \n",
      "2                35.990638  America/Chicago 2010-05-22 19:13:12+00:00   \n",
      "3                12.091107  America/Chicago 2010-05-23 16:50:50+00:00   \n",
      "4                 0.067434  America/Chicago 2010-05-23 17:51:57+00:00   \n",
      "\n",
      "                  local_time  local_time_grouper  \n",
      "0  2010-05-21 21:49:04-05:00 2010-05-21 21:49:04  \n",
      "1  2010-05-22 12:50:55-05:00 2010-05-22 12:50:55  \n",
      "2  2010-05-22 14:13:12-05:00 2010-05-22 14:13:12  \n",
      "3  2010-05-23 11:50:50-05:00 2010-05-23 11:50:50  \n",
      "4  2010-05-23 12:51:57-05:00 2010-05-23 12:51:57  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_gowalla_off['distance_from_prev_check']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Daily Movement Entropy (Average entropy of check in locations over time)**\n",
    "- average shannon entropy of check in locations for each hour of the week\n",
    "- lower the entropy, lower the variability of check ins during that time period\n",
    "- need to recreate the graphs to make sure using correct shannon entropy\n",
    "\n",
    "H(X) = -$\\sum_{i=1}^{n} P(x_i)LogP(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took inspiration from this stack overflow for entropy\n",
    "https://stackoverflow.com/questions/15450192/fastest-way-to-compute-entropy-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Only take users who meet a certain criteria for number of check ins to avoid a bunch of 0 entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we include location ID use this instead of unique loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_entropy(data):\n",
    "    p_data = data.value_counts()\n",
    "    ent = stats.entropy(p_data, base=2)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright2 = df_bright.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright2['weekday'] = df_bright2['local_time_grouper'].map(lambda x: x.weekday())\n",
    "df_bright2['hour'] = df_bright2['local_time_grouper'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bright2_group = df_bright2.groupby(['user','weekday','hour'])\n",
    "#df_bright_group = df_bright.groupby(pd.Grouper(key=\"local_time_grouper\", freq=\"1H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_bright2 = df_bright2_group['unique_loc'].parallel_apply(apply_entropy)\n",
    "entropy_bright2 = entropy_bright2.to_frame().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entropy_bright2 = entropy_bright2[entropy_bright2['unique_loc'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_mean_bright2 = entropy_bright2.drop('user', axis=1).groupby(['weekday','hour']).mean().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_mean_bright2['graph_date'] = entropy_mean_bright2.apply(lambda x: \\\n",
    "                                                          pd.Timestamp(year=2020,month=6,day=int(x['weekday']+1),\\\n",
    "                                                                       hour=int(x['hour'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax1.set(xlabel='', ylabel='Avg Entropy')\n",
    "ax1.plot(entropy_mean_bright2.graph_date, entropy_mean_bright2['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_week_hour.graph_date, entropy_week_hour['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_hour.index, df_bright.Registered, color='b')\n",
    "\n",
    "ax1.xaxis.set(\n",
    "    major_locator=mdates.DayLocator(),\n",
    "    major_formatter=mdates.DateFormatter(\"\\n\\n%A\"),\n",
    "    minor_locator=mdates.HourLocator((6 ,12, 18)),\n",
    "    minor_formatter=mdates.DateFormatter(\"%H\"),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_group = df_gowalla.groupby(pd.Grouper(key=\"local_time_grouper\", freq=\"1H\"))\n",
    "df_bright_group = df_bright.groupby(pd.Grouper(key=\"local_time_grouper\", freq=\"1H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_hour_gowalla = df_gowalla_group['unique_loc'].parallel_apply(apply_entropy)\n",
    "entropy_hour_gowalla = entropy_hour_gowalla.to_frame().reset_index(drop=False)\n",
    "\n",
    "entropy_hour_bright = df_bright_group['unique_loc'].parallel_apply(apply_entropy)\n",
    "entropy_hour_bright = entropy_hour_bright.to_frame().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_hour_gowalla['weekday'] = entropy_hour_gowalla['local_time_grouper'].map(lambda x: x.weekday())\n",
    "entropy_hour_gowalla['hour'] = entropy_hour_gowalla['local_time_grouper'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_hour_bright['weekday'] = entropy_hour_bright['local_time_grouper'].map(lambda x: x.weekday())\n",
    "entropy_hour_bright['hour'] = entropy_hour_bright['local_time_grouper'].map(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_gowalla = entropy_hour_gowalla.groupby(['weekday','hour']).mean()\n",
    "entropy_week_hour_gowalla = entropy_week_hour_gowalla.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_bright = entropy_hour_bright.groupby(['weekday','hour']).mean()\n",
    "entropy_week_hour_bright = entropy_week_hour_bright.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_gowalla['graph_date'] = entropy_week_hour_gowalla.apply(lambda x: \\\n",
    "                                                          pd.Timestamp(year=2020,month=6,day=int(x['weekday']+1),\\\n",
    "                                                                       hour=int(x['hour'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_week_hour_bright['graph_date'] = entropy_week_hour_bright.apply(lambda x: \\\n",
    "                                                          pd.Timestamp(year=2020,month=6,day=int(x['weekday']+1),\\\n",
    "                                                                       hour=int(x['hour'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax1.set(xlabel='', ylabel='Avg Entropy')\n",
    "ax1.plot(entropy_week_hour_gowalla.graph_date, entropy_week_hour_gowalla['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_week_hour.graph_date, entropy_week_hour['unique_loc'], color='g', marker = '.')\n",
    "#ax1.plot(entropy_hour.index, df_bright.Registered, color='b')\n",
    "\n",
    "ax1.xaxis.set(\n",
    "    major_locator=mdates.DayLocator(),\n",
    "    major_formatter=mdates.DateFormatter(\"\\n\\n%A\"),\n",
    "    minor_locator=mdates.HourLocator((6 ,12, 18)),\n",
    "    minor_formatter=mdates.DateFormatter(\"%H\"),\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
